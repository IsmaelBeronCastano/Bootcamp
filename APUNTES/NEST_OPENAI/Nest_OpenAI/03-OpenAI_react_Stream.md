# OpenAI Nest + React - Streams (frontend)

- El caso de uso de pros y cons (sin el stream) es muy similar al de orthography
- Hago un try catch
- Uso fetch con el método POST
- Le paso la url del endpoint
- Configuro fetch con method, los headers
- Le paso el body usando JSOPN.stringify
- Si no hay respuesta.ok lanzo un error
- Si la hay, guardo resp.json en data como ProsConsResponse
- Retorno un objeto con el ok en true y esparzo la data
- En el catch recojo el error
- Retorno un objeto con el ok en false y en content el string de información del error
- En src/core/use-cases/pros-cons.use-case

~~~js
import type { ProsConsResponse } from '../../interfaces';



export const prosConsUseCase = async( prompt: string ) => {

  try {
    
    const resp = await fetch(`${ import.meta.env.VITE_GPT_API }/pros-cons-discusser`, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json'
      },
      body: JSON.stringify({ prompt })
    });

    if ( !resp.ok ) throw new Error('No se pudo realizar la comparación');

    const data = await resp.json() as ProsConsResponse;


    return  {
      ok: true,
      ...data,
    }


  } catch (error) {
    return {
      ok: false,
      content: 'No se pudo realizar la comparación'
    }
  }


}
~~~

- Uso paste JSON as code para sacar la interfaz de la respuesta de OpenAI
- src/interfaces/proscons...

~~~js
// Generated by https://quicktype.io

export interface ProsConsResponse {
  role:    string;
  content: string;
}
~~~

- En presentation/pages/ProsConsPage

~~~js
import { useState } from 'react';
import { GptMessage, MyMessage, TextMessageBox, TypingLoader } from '../../components';
import { prosConsUseCase } from '../../../core/use-cases';


interface Message {
  text: string;
  isGpt: boolean;
}




export const ProsConsPage = () => {

  const [isLoading, setIsLoading] = useState(false);
  const [messages, setMessages] = useState<Message[]>([])


  const handlePost = async( text: string ) => {

    setIsLoading(true);
    setMessages( (prev) => [...prev, { text: text, isGpt: false }] );

    
    const { ok, content } = await prosConsUseCase( text );
    setIsLoading(false);

    if ( !ok ) return;


    setMessages( (prev) => [...prev, { text: content, isGpt: true }] );


  }



  return (
    <div className="chat-container">
      <div className="chat-messages">
        <div className="grid grid-cols-12 gap-y-2">
          {/* Bienvenida */}
          <GptMessage text="Puedes escribir lo que sea que quieres que compare y te de mis puntos de vista." />

          {
            messages.map( (message, index) => (
              message.isGpt
                ? (
                  <GptMessage key={ index } text={ message.text } />
                )
                : (
                  <MyMessage key={ index } text={ message.text } />
                )
                
            ))
          }

          
          {
            isLoading && (
              <div className="col-start-1 col-end-12 fade-in">
                <TypingLoader />
              </div>
            )
          }
          

        </div>
      </div>


      <TextMessageBox 
        onSendMessage={ handlePost }
        placeholder='Escribe aquí lo que deseas'
        disableCorrections
      />

    </div>
  );
};
~~~

- Paso los componentes GptMessage, MyMessage, TYpingLoader¡ y TextMessageBox
- src/components/chat-bubble/GptMessage

~~~js
import Markdown from "react-markdown";

interface Props {
  text: string;
}

export const GptMessage = ({ text }: Props) => {
  return (
    <div className="col-start-1 col-end-9 p-3 rounded-lg">
      <div className="flex flex-row items-start">
        <div className="flex items-center justify-center h-10 w-10 rounded-full bg-green-600 flex-shrink-0">
          G
        </div>
        <div className="relative ml-3 text-sm bg-black bg-opacity-25 pt-3 pb-2 px-4 shadow rounded-xl">
          <Markdown>{text}</Markdown>
        </div>
      </div>
    </div>
  );
};
~~~

- MyMessage

~~~js
interface Props {
  text: string;
}

export const MyMessage = ({ text }: Props) => {
  return (
    <div className="col-start-6 col-end-13 p-3 rounded-lg">
      <div className="flex items-center justify-start flex-row-reverse">
        <div className="flex items-center justify-center h-10 w-10 rounded-full bg-indigo-500 flex-shrink-0">
          F
        </div>
        <div className="relative mr-3 text-sm bg-indigo-700 py-2 px-4 shadow rounded-xl">
          <div>{ text }</div>
        </div>
      </div>
    </div>
  );
};
~~~

- src/components/chat-input/TextMessageBox

~~~js
import { FormEvent, useState } from 'react';


interface Props {
  onSendMessage: (message: string)=>void;
  placeholder?: string;
  disableCorrections?: boolean;
}


export const TextMessageBox = ({ onSendMessage, placeholder, disableCorrections = false }: Props) => {

  const [message, setMessage] = useState('')



  const handleSendMessage = (event: FormEvent<HTMLFormElement>) => {
    event.preventDefault();

    if ( message.trim().length === 0 ) return;

    onSendMessage( message );
    setMessage('');
  }

  return (
    <form
      onSubmit={ handleSendMessage }
      className="flex flex-row items-center h-16 rounded-xl bg-white w-full px-4"
    >

      <div className="flex-grow">
        <div className="relative w-full">

          <input 
            type="text" 
            autoFocus
            name="message"
            className="flex w-full border rounded-xl text-gray-800 focus:outline-none focus:border-indigo-300 pl-4 h-10"
            placeholder={ placeholder }
            autoComplete={ disableCorrections ? 'on': 'off' }
            autoCorrect={ disableCorrections ? 'on': 'off' }
            spellCheck={ disableCorrections ? 'true': 'false' }
            value={ message }
            onChange={ (e) => setMessage( e.target.value ) }
          />


        </div>
      </div>


      <div className="ml-4">
          <button className="btn-primary">
            <span className="mr-2">Enviar</span>
            <i className="fa-regular fa-paper-plane"></i>
          </button>
      </div>




    </form>
  )
}
~~~

- src/components/loaders/TypingLoader

~~~js

.typing {
  display: block;
  width: 60px;
  height: 40px;
  border-radius: 20px;
  margin: 0 1rem;
  display: flex;
  justify-content: center;
  align-items: center;
  background-color: #f2f2f2;
}

.circle {
  display: block;
  height: 10px;
  width: 10px;
  border-radius: 50%;
  background-color: #8d8d8d;
  margin: 3px;
}
.circle.scaling {
  animation: typing 1000ms ease-in-out infinite;
  animation-delay: 3600ms;
}
.circle.bouncing {
  animation: bounce 1000ms ease-in-out infinite;
  animation-delay: 3600ms;
}

.circle:nth-child(1) {
  animation-delay: 0ms;
}

.circle:nth-child(2) {
  animation-delay: 333ms;
}

.circle:nth-child(3) {
  animation-delay: 666ms;
}

@keyframes typing {
  0% {
    transform: scale(1);
  }
  33% {
    transform: scale(1);
  }
  50% {
    transform: scale(1.4);
  }
  100% {
    transform: scale(1);
  }
}

@keyframes bounce {
  0% {
    transform: translateY(0);
  }
  33% {
    transform: translateY(0);
  }
  50% {
    transform: translateY(-10px);
  }
  100% {
    transform: translateY(0);
  }
}
~~~

- El caso de uso de pros-cons-stream.use-case copio el chat-template y le cambio el nombre
- Hago el fetch en un try catch
- Creo el reader
- Creo un decoder para mostrar los mensajes porque la info va a venir poco a poco
- TextDecoder viene en JS
- Creo la variable let text donde iré concatenando lo que el stream me vaya proporcionando
- Del reader con read extraigo el value y done
- Cuando tenga el done es que ha acabado la transmisión, cierro con un break el ciclo while
- En decodedCHunk guardo con decoder.decode el value, debo decirle que esto viene como un stream
- Concateno en text el decodedChunk

~~~js
export const prosConsStreamUseCase = async( prompt: string ) => {

  try {
    
    const resp = await fetch(`${ import.meta.env.VITE_GPT_API }/pros-cons-discusser-stream`, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json'
      },
      body: JSON.stringify({ prompt }),
      // todo: abortSignal
    });

    if ( !resp.ok ) throw new Error('No se pudo realizar la comparación');

    //genero el reader
    const reader = resp.body?.getReader();
    if ( !reader ) {
      console.log('No se pudo generar el reader');
      return null;
    }

    return reader;

    // const decoder = new TextDecoder();

    // let text = '';

    // while( true ) {
    //   const { value, done } = await reader.read();
    //   if ( done ) {
    //     break;
    //   }

    //   const decodedChunk = decoder.decode( value, { stream: true } );
    //   text += decodedChunk;
    //   console.log(text);

    // }




  } catch (error) {
    console.log(error);
    return null;
  }


}
~~~

- Ahora ya tengo la respuesta en consola. debo guardarla en un estado para poder mostrarla en pantalla
- Vamos a ProsCOnsStreamPage
- Comento el decoder en el use-case, lo haré en otro lugar. retorno el reader
- Obtengo el reader haciendo uso del caso de uso
- Lo que quiero es ir mostrando el mensaje en pantall mientras va siendo emitido
- También vamos a programar que si se escribe algo mientras se está retronando una respuesta aborte con AbortSignal
- Una vez obtengo el reader pongo el isLoading en false
- Creo el decoder y el message
- Genero el nuevo mensaje con setMessages con isGpt en true
- Uso un while para controlar el stream y decodificar los chunk
- Tenemos que actualizar el último mensaje (el creado con setMessage), no tenemos que crear uno nuevo
- Actualizo el último mensaje (se refactorizará)

~~~js
import { useRef, useState } from 'react';
import { GptMessage, MyMessage, TypingLoader, TextMessageBox } from '../../components';
import { prosConsStreamGeneratorUseCase } from '../../../core/use-cases';

interface Message {
  text: string;
  isGpt: boolean;
}


export const ProsConsStreamPage = () => {

  const [isLoading, setIsLoading] = useState(false);
  const [messages, setMessages] = useState<Message[]>([])


  const handlePost = async( text: string ) => {


    setIsLoading(true);
    setMessages( (prev) => [...prev, { text: text, isGpt: false }] );

    //Aqui voy a tener el reader
    const reader = prosConsStreamGeneratorUseCase(text);
    setIsLoading(false);//una vez obtengo el reader pongo el isLoading en false

    

    const decoder = new TextDecoder()
    let message = ''

    //genero un nuevo mensaje
    setMessages( (messages) => [ ...messages, { text: '', isGpt: true  } ] );

    while(true){
        const {value, done} = await reader.read()
        if(done){
            break;
        }

        const decodedChunk = decoder.decode(value, {stream: true})
        message+=decodedChunk
    }

        //actualizar el útimo mensaje
        setMessages( (messages) => {
        const newMessages = [...messages]; //esparzo los messages anteriores
        newMessages[ newMessages.length - 1 ].text = message; //length -1 para obtener el último mensaje
                                                //le digo que el texto del último mensaje será igual al mensaje que estoy generando
        return newMessages;                                   
      });

  }



  return (
    <div className="chat-container">
      <div className="chat-messages">
        <div className="grid grid-cols-12 gap-y-2">
          {/* Bienvenida */}
          <GptMessage text="¿Qué deseas comparar hoy?" />

          {
            messages.map( (message, index) => (
              message.isGpt
                ? (
                  <GptMessage key={ index } text={ message.text } />
                )
                : (
                  <MyMessage key={ index } text={ message.text } />
                )
                
            ))
          }

          
          {
            isLoading && (
              <div className="col-start-1 col-end-12 fade-in">
                <TypingLoader />
              </div>
            )
          }
          

        </div>
      </div>


      <TextMessageBox 
        onSendMessage={ handlePost }
        placeholder='Escribe aquí lo que deseas'
        disableCorrections
      />

    </div>
  );
};
~~~

- El componente tiene mucha lógica relacionada a la construcción del mensaje
-----

## Stream con función generadora

- Hago uso del yield para retornar un valor sin cortar el flujo
- Copio el use-case de stream y añado el asterisco a function y en lugar de usar un return uso yield
- pros-cons-stream-generator.use-case

~~~js
//las funciones generadoras llevan un asterisco al final de function
export async function* prosConsStreamGeneratorUseCase( prompt: string)  {

  try {
    
    const resp = await fetch(`${ import.meta.env.VITE_GPT_API }/pros-cons-discusser-stream`, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json'
      },
      body: JSON.stringify({ prompt }),
    });

    if ( !resp.ok ) throw new Error('No se pudo realizar la comparación');

    const reader = resp.body?.getReader();
    if ( !reader ) {
      console.log('No se pudo generar el reader');
      return null;
    }


    const decoder = new TextDecoder();

    let text = '';

    while( true ) {
      const { value, done } = await reader.read();
      if ( done ) {
        break;
      }

      const decodedChunk = decoder.decode( value, { stream: true } );
      text += decodedChunk;
      // console.log(text);
      yield text; //regreso el texto
    }


  } catch (error) {
    console.log(error);
    return null;
  }


}
~~~

- En ProsConsStreamPage ya no necesito el reader, ni el while, etc...
- Llamo al caso de uso y pongo el Loading en false
- Creo el mensaje donde voy a estar haciendo el append de la información
- Uso el for await con el mismo código del setMessages, solo que lo guardo en la variable texto del for
- **Uso de AbortSignals para cancelar el stream**
  - Para cancelar la información generada si mando otra consulta
  - AbortController ya viene en JS
  - Necesito pasárselo al objeto que hace la emisión
  - Uso abortControler.current y uso .signal
  - Le paso al caso de uso ProsConsStreamGeneratorUseCase el abortSignal

~~~js
import { useRef, useState } from 'react';
import { GptMessage, MyMessage, TypingLoader, TextMessageBox } from '../../components';
import { prosConsStreamGeneratorUseCase } from '../../../core/use-cases';

interface Message {
  text: string;
  isGpt: boolean;
}




export const ProsConsStreamPage = () => {

  const abortController = useRef( new AbortController() );
  const isRunning = useRef(false)

  const [isLoading, setIsLoading] = useState(false);
  const [messages, setMessages] = useState<Message[]>([])


  const handlePost = async( text: string ) => {

    if ( isRunning.current ) {
      abortController.current.abort(); //para abortar
      abortController.current = new AbortController();//creo una nueva señal
    }



    setIsLoading(true);
    isRunning.current = true; //en este punto pongo el isRunning en true
    setMessages( (prev) => [...prev, { text: text, isGpt: false }] );

    //TODO: UseCase
    const stream = prosConsStreamGeneratorUseCase( text, abortController.current.signal ); //le paso el signal para que cuando reciba esta señal cancele
    setIsLoading(false);
    

    setMessages( (messages) => [ ...messages, { text: '', isGpt: true  } ] );

    for await (const text of stream) {
      setMessages( (messages) => {
        const newMessages = [...messages];
        newMessages[ newMessages.length - 1 ].text = text;
        return newMessages;
      });
    }

    isRunning.current = false; //acabado el trabajo lo pongo a false

  }



  return (
    <div className="chat-container">
      <div className="chat-messages">
        <div className="grid grid-cols-12 gap-y-2">
          {/* Bienvenida */}
          <GptMessage text="¿Qué deseas comparar hoy?" />

          {
            messages.map( (message, index) => (
              message.isGpt
                ? (
                  <GptMessage key={ index } text={ message.text } />
                )
                : (
                  <MyMessage key={ index } text={ message.text } />
                )
                
            ))
          }

          
          {
            isLoading && (
              <div className="col-start-1 col-end-12 fade-in">
                <TypingLoader />
              </div>
            )
          }
          

        </div>
      </div>


      <TextMessageBox 
        onSendMessage={ handlePost }
        placeholder='Escribe aquí lo que deseas'
        disableCorrections
      />

    </div>
  );
};
~~~

- ProsConsStreamGeneratorUseCase
- Necesito pasarle al fetch el abortSignal

~~~js

export async function* prosConsStreamGeneratorUseCase( prompt: string, abortSignal: AbortSignal )  {

  try {
    
    const resp = await fetch(`${ import.meta.env.VITE_GPT_API }/pros-cons-discusser-stream`, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json'
      },
      body: JSON.stringify({ prompt }),
      signal: abortSignal, //le paso el abortSignal
    });

    if ( !resp.ok ) throw new Error('No se pudo realizar la comparación');

    const reader = resp.body?.getReader();
    if ( !reader ) {
      console.log('No se pudo generar el reader');
      return null;
    }


    const decoder = new TextDecoder();

    let text = '';

    while( true ) {
      const { value, done } = await reader.read();
      if ( done ) {
        break;
      }

      const decodedChunk = decoder.decode( value, { stream: true } );
      text += decodedChunk;
      // console.log(text);
      yield text;
    }


  } catch (error) {
    console.log(error);
    return null;
  }


}
~~~